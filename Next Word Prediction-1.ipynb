{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:   No, like I said, I'm not going to really concern myself with that. Not till later.\n",
      "\n",
      "The Last Line:   From his mom...\n"
     ]
    }
   ],
   "source": [
    "filename = \"lazar\"\n",
    "exist = os.path.isfile(\"corpus-\"+filename+\"-cleaned.txt\")\n",
    "if not exist:\n",
    "    file = open(filename + \".txt\", \"r\", encoding = \"utf8\")\n",
    "    lines = []\n",
    "    l = 0\n",
    "    for i in file:\n",
    "        lines.append(i)\n",
    "        l = l + 1\n",
    "    if (l > 5):\n",
    "        print(\"The First Line: \", lines[0])\n",
    "        print(\"The Last Line: \", lines[-1])\n",
    "else:\n",
    "    file = open(\"corpus-\"+filename+\"-cleaned.txt\", \"r\", encoding = \"utf8\")\n",
    "    lines = []\n",
    "    for i in file:\n",
    "        lines.append(i)\n",
    "    data = lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" No, like I said, I'm not going to really concern myself with that. Not till later.  I wasn't told what I was working on...  At that point... no.  Yeah.  The best thing for me to do is to pull out...  I have these calendars... they're big wall calendars where I write what happened every day on it... and I've had them since like 1980... so I have exact dates.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not exist:\n",
    "    data = \"\"\n",
    "\n",
    "    for i in lines:\n",
    "        data = ' '. join(lines)\n",
    "\n",
    "    data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "    \n",
    "    \n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No like I said m not going to really concern myself with that Not till later wasn t told what was working on At point no Yeah The best thing for me do is pull out have these calendars they re big wall where write happened every day it and ve had them since 1980 so exact dates And photocopied some of those can remember save my life even within a year EG G this would be second one But the first different job Five hesitate because don know One did Dennis who wound up being supervisor still he looke'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not exist:\n",
    "    import string\n",
    "\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    data = data.translate(translator)\n",
    "\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, like I said, I'm not going to really concern myself with that. Not till later. wasn't told what was working on... At that point... no. Yeah. The best thing for me do is pull out... have these calendars... they're big wall calendars where write happened every day on it... and I've had them since 1980... so exact dates. And photocopied some of those so... can't remember dates save my life... even within a year. EG&G. Yeah, this would be second one. But the first one different job. Five? No. Ye\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not exist:\n",
    "    z = []\n",
    "\n",
    "    for i in data.split():\n",
    "        if i not in z:\n",
    "            z.append(i)\n",
    "\n",
    "    data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exist:\n",
    "    file1 = open(\"corpus-\"+filename+\"-cleaned.txt\",\"w\", encoding = \"utf8\")\n",
    "    file1.write(data)\n",
    "    file1.close() #to change file access modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 49, 1, 105, 1100, 23, 272, 50, 51, 1101]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer-'+filename+'.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3348\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  5431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   4,   49],\n",
       "       [  49,    1],\n",
       "       [   1,  105],\n",
       "       [ 105, 1100],\n",
       "       [1100,   23],\n",
       "       [  23,  272],\n",
       "       [ 272,   50],\n",
       "       [  50,   51],\n",
       "       [  51, 1101],\n",
       "       [1101,  273]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [   4   49    1  105 1100]\n",
      "The responses are:  [  49    1  105 1100   23]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Platinum\\ana3\\envs\\arioo5\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Platinum\\ana3\\envs\\arioo5\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             33480     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1, 1000)           4044000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000)              8004000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3348)              3351348   \n",
      "=================================================================\n",
      "Total params: 16,433,828\n",
      "Trainable params: 16,433,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\Platinum\\ana3\\envs\\arioo5\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19776\\3869421814.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana3\\envs\\arioo5\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana3\\envs\\arioo5\\lib\\site-packages\\keras\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana3\\envs\\arioo5\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayout_map\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana3\\envs\\arioo5\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\Platinum\\ana3\\envs\\arioo5\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword-\"+filename+\".h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword'+filename\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Platinum\\ana3\\envs\\arioo5\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 8.1217\n",
      "Epoch 00001: loss improved from inf to 8.12181, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 6s 1ms/sample - loss: 8.1218\n",
      "Epoch 2/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 8.0975\n",
      "Epoch 00002: loss improved from 8.12181 to 8.09731, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 2s 300us/sample - loss: 8.0973\n",
      "Epoch 3/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 8.0356\n",
      "Epoch 00003: loss improved from 8.09731 to 8.03597, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 228us/sample - loss: 8.0360\n",
      "Epoch 4/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 7.8487\n",
      "Epoch 00004: loss improved from 8.03597 to 7.84851, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 274us/sample - loss: 7.8485\n",
      "Epoch 5/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 7.6467\n",
      "Epoch 00005: loss improved from 7.84851 to 7.64801, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 239us/sample - loss: 7.6480\n",
      "Epoch 6/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 7.4745\n",
      "Epoch 00006: loss improved from 7.64801 to 7.47736, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 261us/sample - loss: 7.4774\n",
      "Epoch 7/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 7.3241\n",
      "Epoch 00007: loss improved from 7.47736 to 7.32727, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 242us/sample - loss: 7.3273\n",
      "Epoch 8/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 7.0924\n",
      "Epoch 00008: loss improved from 7.32727 to 7.09327, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 247us/sample - loss: 7.0933\n",
      "Epoch 9/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 6.8332\n",
      "Epoch 00009: loss improved from 7.09327 to 6.83676, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 236us/sample - loss: 6.8368\n",
      "Epoch 10/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 6.5864- ETA: 0s - loss: 6\n",
      "Epoch 00010: loss improved from 6.83676 to 6.58914, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 243us/sample - loss: 6.5891\n",
      "Epoch 11/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 6.3299\n",
      "Epoch 00011: loss improved from 6.58914 to 6.33604, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 244us/sample - loss: 6.3360\n",
      "Epoch 12/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 6.0635\n",
      "Epoch 00012: loss improved from 6.33604 to 6.07096, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 247us/sample - loss: 6.0710\n",
      "Epoch 13/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 5.7689\n",
      "Epoch 00013: loss improved from 6.07096 to 5.78357, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 272us/sample - loss: 5.7836\n",
      "Epoch 14/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 5.5036\n",
      "Epoch 00014: loss improved from 5.78357 to 5.51168, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 257us/sample - loss: 5.5117\n",
      "Epoch 15/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 5.2194\n",
      "Epoch 00015: loss improved from 5.51168 to 5.22820, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 255us/sample - loss: 5.2282\n",
      "Epoch 16/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 4.9623\n",
      "Epoch 00016: loss improved from 5.22820 to 4.96716, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 272us/sample - loss: 4.9672\n",
      "Epoch 17/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 4.7387\n",
      "Epoch 00017: loss improved from 4.96716 to 4.74452, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 250us/sample - loss: 4.7445\n",
      "Epoch 18/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 4.5018\n",
      "Epoch 00018: loss improved from 4.74452 to 4.50589, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 247us/sample - loss: 4.5059\n",
      "Epoch 19/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 4.2982\n",
      "Epoch 00019: loss improved from 4.50589 to 4.30013, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 263us/sample - loss: 4.3001\n",
      "Epoch 20/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 4.1335\n",
      "Epoch 00020: loss improved from 4.30013 to 4.13418, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 268us/sample - loss: 4.1342\n",
      "Epoch 21/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 3.9476\n",
      "Epoch 00021: loss improved from 4.13418 to 3.94727, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 223us/sample - loss: 3.9473\n",
      "Epoch 22/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 3.7628\n",
      "Epoch 00022: loss improved from 3.94727 to 3.77234, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 259us/sample - loss: 3.7723\n",
      "Epoch 23/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 3.6515\n",
      "Epoch 00023: loss improved from 3.77234 to 3.65154, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 2s 295us/sample - loss: 3.6515\n",
      "Epoch 24/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 3.5054\n",
      "Epoch 00024: loss improved from 3.65154 to 3.50882, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 268us/sample - loss: 3.5088\n",
      "Epoch 25/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 3.3929\n",
      "Epoch 00025: loss improved from 3.50882 to 3.40273, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 271us/sample - loss: 3.4027\n",
      "Epoch 26/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 3.2659\n",
      "Epoch 00026: loss improved from 3.40273 to 3.28309, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 241us/sample - loss: 3.2831\n",
      "Epoch 27/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 3.1983\n",
      "Epoch 00027: loss improved from 3.28309 to 3.19886, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 2s 283us/sample - loss: 3.1989\n",
      "Epoch 28/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 3.0659\n",
      "Epoch 00028: loss improved from 3.19886 to 3.08047, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 237us/sample - loss: 3.0805\n",
      "Epoch 29/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 3.0113\n",
      "Epoch 00029: loss improved from 3.08047 to 3.01624, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 259us/sample - loss: 3.0162\n",
      "Epoch 30/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 2.8900\n",
      "Epoch 00030: loss improved from 3.01624 to 2.90913, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 248us/sample - loss: 2.9091\n",
      "Epoch 31/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 2.8163\n",
      "Epoch 00031: loss improved from 2.90913 to 2.82461, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 241us/sample - loss: 2.8246\n",
      "Epoch 32/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 2.7291\n",
      "Epoch 00032: loss improved from 2.82461 to 2.75585, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 218us/sample - loss: 2.7558\n",
      "Epoch 33/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 2.7041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00033: loss improved from 2.75585 to 2.71753, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 237us/sample - loss: 2.7175\n",
      "Epoch 34/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 2.6233\n",
      "Epoch 00034: loss improved from 2.71753 to 2.63083, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 234us/sample - loss: 2.6308\n",
      "Epoch 35/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 2.5671\n",
      "Epoch 00035: loss improved from 2.63083 to 2.57521, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 234us/sample - loss: 2.5752\n",
      "Epoch 36/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 2.4976\n",
      "Epoch 00036: loss improved from 2.57521 to 2.50144, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 2s 302us/sample - loss: 2.5014\n",
      "Epoch 37/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 2.4361\n",
      "Epoch 00037: loss improved from 2.50144 to 2.43931, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 245us/sample - loss: 2.4393\n",
      "Epoch 38/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 2.3657\n",
      "Epoch 00038: loss improved from 2.43931 to 2.37292, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 262us/sample - loss: 2.3729\n",
      "Epoch 39/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 2.2822\n",
      "Epoch 00039: loss improved from 2.37292 to 2.29479, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 255us/sample - loss: 2.2948\n",
      "Epoch 40/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 2.2297\n",
      "Epoch 00040: loss improved from 2.29479 to 2.24218, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 236us/sample - loss: 2.2422\n",
      "Epoch 41/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 2.1702\n",
      "Epoch 00041: loss improved from 2.24218 to 2.18026, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 253us/sample - loss: 2.1803\n",
      "Epoch 42/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 2.1034\n",
      "Epoch 00042: loss improved from 2.18026 to 2.11096, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 245us/sample - loss: 2.1110\n",
      "Epoch 43/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 2.0265\n",
      "Epoch 00043: loss improved from 2.11096 to 2.03347, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 251us/sample - loss: 2.0335\n",
      "Epoch 44/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 2.0094\n",
      "Epoch 00044: loss improved from 2.03347 to 2.01065, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 241us/sample - loss: 2.0107\n",
      "Epoch 45/50\n",
      "5120/5431 [===========================>..] - ETA: 0s - loss: 1.9708\n",
      "Epoch 00045: loss improved from 2.01065 to 1.98485, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 235us/sample - loss: 1.9849\n",
      "Epoch 46/50\n",
      "5184/5431 [===========================>..] - ETA: 0s - loss: 1.8992\n",
      "Epoch 00046: loss improved from 1.98485 to 1.91428, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 238us/sample - loss: 1.9143\n",
      "Epoch 47/50\n",
      "5376/5431 [============================>.] - ETA: 0s - loss: 1.8704\n",
      "Epoch 00047: loss improved from 1.91428 to 1.87024, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 272us/sample - loss: 1.8702\n",
      "Epoch 48/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 1.8388\n",
      "Epoch 00048: loss improved from 1.87024 to 1.84621, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 252us/sample - loss: 1.8462\n",
      "Epoch 49/50\n",
      "5248/5431 [===========================>..] - ETA: 0s - loss: 1.7730\n",
      "Epoch 00049: loss improved from 1.84621 to 1.78320, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 247us/sample - loss: 1.7832\n",
      "Epoch 50/50\n",
      "5312/5431 [============================>.] - ETA: 0s - loss: 1.7250\n",
      "Epoch 00050: loss improved from 1.78320 to 1.73147, saving model to nextword-lazar.h5\n",
      "5431/5431 [==============================] - 1s 243us/sample - loss: 1.7315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x221e7b03848>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\n",
    "# tensorboard --logdir=\"./logsnextword1\"\n",
    "# http://DESKTOP-U3TSCVT:6006/\n",
    "\n",
    "#from IPython.display import Image \n",
    "#pil_img = Image(filename='graph1.png')\n",
    "#display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "### We are able to develop a decent next word prediction model and are able to get a declining loss and an overall decent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    #print(text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = np.array(sequence)\n",
    "    res = []\n",
    "    preds = model.predict(sequence)\n",
    "    preds1= np.flip(np.argsort(preds,axis=1))\n",
    "    preds = np.argmax(preds,axis=1)\n",
    "    res.append((preds1[0][0]))\n",
    "    res.append((preds1[0][1]))\n",
    "    res.append((preds1[0][2]))\n",
    "    print(len(res))\n",
    "    predicted_word = \"\"\n",
    "    predicted_words = []\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value in res:\n",
    "            predicted_words.append(key)\n",
    "            #print(key)\n",
    "            #break\n",
    "\n",
    "    print(len(predicted_word))\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "['weapons', 'grill', 'attempt']\n"
     ]
    }
   ],
   "source": [
    "text = \"Obviously\";\n",
    "export = Predict_Next_Words(model, tokenizer, text)\n",
    "print(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: call\n",
      "1572\n",
      "3177\n",
      "1330\n",
      "[1572]\n",
      "234\n",
      "2\n",
      "51\n",
      "[234]\n",
      "784\n",
      "754\n",
      "441\n",
      "[784]\n",
      "28\n",
      "601\n",
      "83\n",
      "[28]\n",
      "1976\n",
      "1649\n",
      "272\n",
      "[1976]\n",
      "79\n",
      "522\n",
      "546\n",
      "[79]\n",
      "1198\n",
      "929\n",
      "1431\n",
      "[1198]\n",
      "59\n",
      "86\n",
      "250\n",
      "[59]\n",
      "170\n",
      "1107\n",
      "781\n",
      "[170]\n",
      "1969\n",
      "2060\n",
      "325\n",
      "[1969]\n",
      " call parasitic guards understand where lunchroom more than them good\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "text = input(\"Enter your line: \")\n",
    "sentence = \"\"\n",
    "while(c<10):\n",
    "    sentence = sentence + \" \" + text\n",
    "\n",
    "    #if text == \"stop\":\n",
    "        #print(\"Ending The Program.....\")\n",
    "        #break\n",
    "    \n",
    "    #else:\n",
    "    try:\n",
    "        #text = text.split(\" \")\n",
    "        #text = text[-1]\n",
    "\n",
    "        #text = ''.join(text)\n",
    "        text = Predict_Next_Words(model, tokenizer, text)\n",
    "        c = c + 1\n",
    "    except:\n",
    "        continue\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [0]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matrix([[1,2,3,33],[88,5,6,66],[7,8,9,99]])\n",
    "\n",
    "s= np.argmax(A,axis=1)  # 11, which is the position of 99\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
